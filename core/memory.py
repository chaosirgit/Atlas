import os
import json
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path
import dashscope
from dashscope import TextEmbedding

# å…ˆæ£€æŸ¥numpyæ˜¯å¦å®‰è£…
try:
    import numpy as np

    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False
    print("âš ï¸ numpyæœªå®‰è£…ï¼Œé•¿æœŸè®°å¿†åŠŸèƒ½å°†è¢«ç¦ç”¨")


class Memory:
    """æ”¹è¿›çš„è®°å¿†ç³»ç»Ÿ - çŸ­æœŸè®°å¿† + é•¿æœŸè®°å¿†"""

    def __init__(self, workspace: str = "atlas_workspace"):
        self.workspace = Path(workspace)
        self.memory_dir = self.workspace / "memory"
        self.memory_dir.mkdir(parents=True, exist_ok=True)

        # çŸ­æœŸè®°å¿†æ–‡ä»¶
        self.short_term_file = self.memory_dir / "short_term_memory.json"

        # çŸ­æœŸè®°å¿†ï¼ˆæœ€è¿‘20æ¡å¯¹è¯ï¼‰
        self.short_term_memory: List[Dict[str, Any]] = self._load_short_term()  # æ”¹è¿™é‡Œ
        self.max_short_term = 20

        # é•¿æœŸè®°å¿†æ–‡ä»¶
        self.long_term_file = self.memory_dir / "long_term_memory.json"
        self.embeddings_file = self.memory_dir / "embeddings.npy"

        # åŠ è½½è®°å¿†
        self.long_term_memory: List[Dict[str, Any]] = self._load_long_term()
        self.embeddings: Optional[np.ndarray] = self._load_embeddings() if HAS_NUMPY else None

        print(f"âœ… è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"   çŸ­æœŸè®°å¿†: {len(self.short_term_memory)}æ¡")
        print(f"   é•¿æœŸè®°å¿†: {len(self.long_term_memory)}æ¡")

    def _load_short_term(self) -> List[Dict[str, Any]]:
        """åŠ è½½çŸ­æœŸè®°å¿†"""
        try:
            if self.short_term_file.exists():
                with open(self.short_term_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # print(f"ğŸ“‚ åŠ è½½äº† {len(data)} æ¡çŸ­æœŸè®°å¿†")
                    return data
        except Exception as e:
            print(f"âš ï¸ åŠ è½½çŸ­æœŸè®°å¿†å¤±è´¥: {e}")
        return []

    def _save_short_term(self):
        """ä¿å­˜çŸ­æœŸè®°å¿†"""
        try:
            with open(self.short_term_file, 'w', encoding='utf-8') as f:
                json.dump(self.short_term_memory, f, ensure_ascii=False, indent=2)
            # print(f"ğŸ’¾ ä¿å­˜äº† {len(self.short_term_memory)} æ¡çŸ­æœŸè®°å¿†")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜çŸ­æœŸè®°å¿†å¤±è´¥: {e}")

    def _load_long_term(self) -> List[Dict[str, Any]]:
        """åŠ è½½é•¿æœŸè®°å¿†"""
        try:
            if self.long_term_file.exists():
                with open(self.long_term_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # print(f"ğŸ“‚ åŠ è½½äº† {len(data)} æ¡é•¿æœŸè®°å¿†")
                    return data
        except Exception as e:
            print(f"âš ï¸ åŠ è½½é•¿æœŸè®°å¿†å¤±è´¥: {e}")
        return []

    def _save_long_term(self):
        """ä¿å­˜é•¿æœŸè®°å¿†"""
        try:
            with open(self.long_term_file, 'w', encoding='utf-8') as f:
                json.dump(self.long_term_memory, f, ensure_ascii=False, indent=2)
            # print(f"ğŸ’¾ ä¿å­˜äº† {len(self.long_term_memory)} æ¡é•¿æœŸè®°å¿†")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜é•¿æœŸè®°å¿†å¤±è´¥: {e}")

    def _load_embeddings(self) -> Optional[np.ndarray]:
        """åŠ è½½å‘é‡"""
        if not HAS_NUMPY:
            return None

        try:
            if self.embeddings_file.exists():
                data = np.load(self.embeddings_file)
                # print(f"ğŸ“Š åŠ è½½äº† {len(data)} ä¸ªå‘é‡")
                return data
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å‘é‡å¤±è´¥: {e}")
        return None

    def _save_embeddings(self):
        """ä¿å­˜å‘é‡"""
        if not HAS_NUMPY or self.embeddings is None:
            return

        try:
            np.save(self.embeddings_file, self.embeddings)
            # print(f"ğŸ’¾ ä¿å­˜äº† {len(self.embeddings)} ä¸ªå‘é‡")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å‘é‡å¤±è´¥: {e}")

    def _get_embedding(self, text: str) -> Optional[List[float]]:
        """è·å–æ–‡æœ¬å‘é‡"""
        if not HAS_NUMPY:
            return None

        try:
            response = TextEmbedding.call(
                model=TextEmbedding.Models.text_embedding_v1,
                input=text
            )
            return response.output['embeddings'][0]['embedding']
        except Exception as e:
            print(f"âš ï¸ å‘é‡åŒ–å¤±è´¥: {e}")
            return None

    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°çŸ­æœŸè®°å¿†"""
        message = {
            'role': role,
            'content': content,
            'timestamp': datetime.now().isoformat()
        }

        self.short_term_memory.append(message)

        # ç«‹å³ä¿å­˜çŸ­æœŸè®°å¿†
        self._save_short_term()

        # å¦‚æœçŸ­æœŸè®°å¿†æ»¡äº†ï¼Œè½¬ç§»åˆ°é•¿æœŸè®°å¿†
        if len(self.short_term_memory) > self.max_short_term:
            self._move_to_long_term()

    def _move_to_long_term(self):
        """å°†æ—§çš„çŸ­æœŸè®°å¿†è½¬ç§»åˆ°é•¿æœŸè®°å¿†"""
        if not HAS_NUMPY:
            # å¦‚æœæ²¡æœ‰numpyï¼Œåªä¿ç•™æœ€è¿‘çš„å¯¹è¯
            self.short_term_memory = self.short_term_memory[-self.max_short_term:]
            return

        # å–å‡ºæœ€è€çš„å¯¹è¯å¯¹ï¼ˆuser + assistantï¼‰
        if len(self.short_term_memory) >= 2:
            old_messages = self.short_term_memory[:2]
            self.short_term_memory = self.short_term_memory[2:]

            # åˆå¹¶æˆä¸€æ¡è®°å½•
            conversation = {
                'user': old_messages[0]['content'],
                'assistant': old_messages[1]['content'] if len(old_messages) > 1 else '',
                'timestamp': old_messages[0]['timestamp']
            }

            # ç”Ÿæˆå‘é‡
            text = f"ç”¨æˆ·: {conversation['user']}\nAtlas: {conversation['assistant']}"
            embedding = self._get_embedding(text)

            if embedding:
                self.long_term_memory.append(conversation)

                # æ·»åŠ å‘é‡
                if self.embeddings is None:
                    self.embeddings = np.array([embedding])
                else:
                    self.embeddings = np.vstack([self.embeddings, embedding])

                # ä¿å­˜
                self._save_long_term()
                self._save_embeddings()

    def search_memory(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """åœ¨é•¿æœŸè®°å¿†ä¸­æœç´¢ç›¸å…³å¯¹è¯"""
        if not HAS_NUMPY or len(self.long_term_memory) == 0 or self.embeddings is None:
            return []

        # è·å–æŸ¥è¯¢å‘é‡
        query_embedding = self._get_embedding(query)
        if not query_embedding:
            return []

        query_vec = np.array(query_embedding)

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for i, emb in enumerate(self.embeddings):
            sim = self._cosine_similarity(query_vec, emb)
            similarities.append((i, sim))

        # æ’åºå¹¶è¿”å›top_k
        similarities.sort(key=lambda x: x[1], reverse=True)
        results = []
        for i, sim in similarities[:top_k]:
            if sim > 0.7:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                results.append(self.long_term_memory[i])

        return results

    def format_for_qwen(self, include_long_term: bool = True, query: str = None) -> List[Dict[str, str]]:
        """æ ¼å¼åŒ–è®°å¿†ä¾›åƒé—®ä½¿ç”¨"""
        messages = []

        # å¦‚æœæœ‰æŸ¥è¯¢ï¼Œæœç´¢ç›¸å…³çš„é•¿æœŸè®°å¿†
        if include_long_term and query and len(self.long_term_memory) > 0:
            relevant_memories = self.search_memory(query, top_k=3)
            if relevant_memories:
                context = "ä»¥ä¸‹æ˜¯ç›¸å…³çš„å†å²å¯¹è¯ï¼š\n"
                for mem in relevant_memories:
                    context += f"ç”¨æˆ·: {mem['user']}\nAtlas: {mem['assistant']}\n\n"
                messages.append({'role': 'system', 'content': context})

        # æ·»åŠ çŸ­æœŸè®°å¿†
        for msg in self.short_term_memory:
            messages.append({
                'role': msg['role'],
                'content': msg['content']
            })

        return messages

    def get_summary(self) -> str:
        """è·å–è®°å¿†æ‘˜è¦"""
        return f"çŸ­æœŸè®°å¿†: {len(self.short_term_memory)}æ¡ | é•¿æœŸè®°å¿†: {len(self.long_term_memory)}æ¡"

    def clear_memory(self):
        """æ¸…ç©ºæ‰€æœ‰è®°å¿†"""
        self.short_term_memory = []
        self.long_term_memory = []
        self.embeddings = None

        try:
            if self.short_term_file.exists():
                self.short_term_file.unlink()
            if self.long_term_file.exists():
                self.long_term_file.unlink()
            if self.embeddings_file.exists():
                self.embeddings_file.unlink()
            print("ğŸ—‘ï¸ è®°å¿†å·²æ¸…ç©º")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç©ºè®°å¿†å¤±è´¥: {e}")

    def save_all(self):
        """ä¿å­˜æ‰€æœ‰è®°å¿†ï¼ˆç¨‹åºé€€å‡ºæ—¶è°ƒç”¨ï¼‰"""
        self._save_short_term()
        if len(self.long_term_memory) > 0:
            self._save_long_term()
        if self.embeddings is not None:
            self._save_embeddings()
        print("ğŸ’¾ æ‰€æœ‰è®°å¿†å·²ä¿å­˜")
